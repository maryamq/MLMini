{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Fight The Risk of Cancer!\n",
      "http://www.adclick.ws/p.cfm?o=315&s=pk007\n",
      "\n",
      "2) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\n",
      "http://www.adclick.ws/p.cfm?o=249&s=pk007\n",
      "\n",
      "3) Get the Child Support You Deserve - Free Legal Advice\n",
      "http://www.adclick.ws/p.cfm?o=245&s=pk002\n",
      "\n",
      "4) Join the Web's Fastest Growing Singles Community\n",
      "http://www.adclick.ws/p.cfm?o=259&s=pk007\n",
      "\n",
      "5) Start Your Private Photo Album Online!\n",
      "http://www.adclick.ws/p.cfm?o=283&s=pk007\n",
      "\n",
      "Have a Wonderful Day,\n",
      "Offer Manager\n",
      "PrizeMama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you wish to leave this list please use the link below.\n",
      "http://www.qves.com/trim/?ilug@linux.ie%7C17%7C114258\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(text/plain, text/enriched)', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart/alternative', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(spam_emails + ham_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "<HTML><HEAD><TITLE>Hi i'm Rita !!!</TITLE>\n",
      "<META http-equiv=Content-Type content=\"text/html; charset=windows-1252\">\n",
      "<META content=\"Microsoft FrontPage 4.0\" name=GENERATOR></HEAD>\n",
      "<BODY bgcolor=\"#FF00FF\" link=\"#800000\" vlink=\"#800000\" alink=\"#800000\" text=\"#FF0000\"><LEFT>\n",
      "<TABLE width=427 height=\"60\">\n",
      "  <TBODY>\n",
      "  <TR>\n",
      "    <TD align=center bgcolor=\"#FF00FF\" width=\"419\" height=\"56\"><b><a href=\"http://www.amsterdamcash.com/click.cfm?siteid=0017&amp;companyid=33043\"><font color=\"#800000\"><span style=\"background-color: #FFFFFF\"><font face=\"Times New Roman\" size=\"7\"><i>R</i></font></span><font face=\"verdana\" size=\"6\"><span style=\"background-color: #00FFFF\">E</span><span style=\"background-color: #FFFF00\">A</span></font><font face=\"Times New Roman\" size=\"6\"><span style=\"background-color: #00FF00\">D</span></font><font face=\"verdana\" size=\"6\">\n",
      "      </font><span style=\"background-color: #FFFFFF\"><font face=\"Tahoma\" size=\"7\"><i>M</i></font></span><font face=\"Lucida Console\" size=\"6\"><span style=\"b ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==0]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "print(len(html_spam_emails))\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     HYPERLINK READ\n",
      "      MY\n",
      "      LIPStick\n",
      "       !\n",
      "       HYPERLINK\n",
      "       HYPERLINK LIVE\n",
      "      F ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => Comput\n",
      "Computation => Comput\n",
      "Computing => Comput\n",
      "Computed => Comput\n",
      "Compute => Comput\n",
      "Compulsive => Compuls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'NUMBER': 6, 'report': 1, 'win': 1, 'NUMBERtNUMB': 1, 'donal': 1, 'date': 1, 'url': 1, 'case': 1, 'libel': 1, 'bbc': 1, 'against': 1, 'URL': 1, 'polic': 1, 'profil': 1, 'macintyr': 1, 'high': 1}),\n",
       "       Counter({'URL': 4, 'i': 3, 'that': 2, 'in': 2, 'if': 2, 'befor': 2, 'it': 2, 'waider': 2, 'me': 1, 'steal': 1, 'beg': 1, 's': 1, 'linux': 1, 'probabl': 1, 'subscript': 1, 'leav': 1, 'cheer': 1, 'there': 1, 'you': 1, 'maintain': 1, 'veri': 1, 'list': 1, 'person': 1, 'get': 1, 'well': 1, 'for': 1, 'is': 1, 'borrow': 1, 'not': 1, 'they': 1, 'far': 1, 'truli': 1, 'ilug': 1, 'your': 1, 'gone': 1, 'fact': 1, 'depart': 1, 'say': 1, 'back': 1, 're': 1, 'much': 1, 'just': 1, 'too': 1, 'can': 1, 'zawinski': 1, 'a': 1, 'she': 1, 'ye': 1, 'lbw': 1, 'inform': 1, 'the': 1, 'we': 1, 'doom': 1, 'realiz': 1, 'user': 1, 'un': 1, 'm': 1, 'irish': 1, 'fun': 1, 'of': 1, 'and': 1, 'jami': 1, 'good': 1, 'are': 1, 'or': 1, 'start': 1, 'doolin': 1, 'folk': 1, 'listmast': 1, 'group': 1, 'now': 1, 'way': 1, 'head': 1}),\n",
       "       Counter({'and': 18, 'of': 16, 'to': 15, 'i': 14, 'the': 14, 'my': 13, 'in': 11, 'you': 9, 'thi': 9, 'for': 7, 'want': 5, 'your': 5, 'husband': 5, 'that': 5, 'other': 4, 'URL': 4, 'will': 4, 'NUMBER': 4, 'us': 4, 'son': 4, 'money': 3, 'not': 3, 's': 3, 'late': 3, 'kabila': 3, 'situat': 3, 'assist': 3, 'where': 3, 'countri': 3, 'invest': 3, 'as': 3, 'is': 3, 'deposit': 3, 'with': 3, 'mr': 3, 'kongolo': 3, 'safe': 3, 'decid': 2, 'purpos': 2, 'out': 2, 'govern': 2, 'move': 2, 'so': 2, 'fund': 2, 'our': 2, 'also': 2, 'congo': 2, 'chang': 2, 'social': 2, 'swiss': 2, 'am': 2, 'project': 2, 'presid': 2, 'inform': 2, 'confid': 2, 'dollar': 2, 'compani': 2, 'can': 2, 'which': 2, 'be': 2, 'european': 2, 'state': 2, 'seko': 2, 'drc': 2, 'sese': 2, 'ident': 2, 'confisc': 2, 'maintain': 2, 'ha': 2, 'democrat': 2, 'wa': 2, 'settl': 2, 'republ': 2, 'later': 2, 'we': 2, 'all': 2, 'now': 2, 'point': 1, 'abov': 1, 'some': 1, 'abidjan': 1, 'diseas': 1, 'may': 1, 'irish': 1, 'properti': 1, 'have': 1, 'henc': 1, 'treasur': 1, 'such': 1, 'like': 1, 'betray': 1, 'on': 1, 'chateaux': 1, 'real': 1, 'user': 1, 'unit': 1, 'till': 1, 'along': 1, 'what': 1, 'specul': 1, 'multi': 1, 'zair': 1, 'demand': 1, 'dear': 1, 'servic': 1, 'due': 1, 'valid': 1, 'address': 1, 'brief': 1, 'better': 1, 'hope': 1, 'made': 1, 'studi': 1, 'a': 1, 'children': 1, 'acknowledg': 1, 'level': 1, 'while': 1, 'bank': 1, 'person': 1, 'contact': 1, 'write': 1, 'over': 1, 'africa': 1, 'mobutu': 1, 'letter': 1, 'take': 1, 'repos': 1, 'current': 1, 'present': 1, 'joseph': 1, 'friend': 1, 'reason': 1, 'franc': 1, 'mariam': 1, 'claim': 1, 'conclus': 1, 'who': 1, 'number': 1, 'head': 1, 'stock': 1, 'land': 1, 'laurent': 1, 'two': 1, 'at': 1, 'ivoir': 1, 'do': 1, 'jame': 1, 'receipt': 1, 'cote': 1, 'eighteen': 1, 'telephon': 1, 'low': 1, 'french': 1, 'non': 1, 'southern': 1, 'subscript': 1, 'kindli': 1, 'introduc': 1, 'sum': 1, 'nation': 1, 'yourinterest': 1, 'dr': 1, 'remuner': 1, 'emphas': 1, 'upcom': 1, 'morroco': 1, 'most': 1, 'email': 1, 'linux': 1, 'mail': 1, 'acquir': 1, 'if': 1, 'divulgeto': 1, 'behalf': 1, 'receiv': 1, 'lay': 1, 'regard': 1, 'indic': 1, 'secur': 1, 'code': 1, 'tell': 1, 'thing': 1, 'fax': 1, 'modal': 1, 'un': 1, 'm': 1, 'one': 1, 'freez': 1, 'widow': 1, 'good': 1, 'consid': 1, 'shall': 1, 'new': 1, 'reveal': 1, 'keep': 1, 'by': 1, 'event': 1, 'million': 1, 'trace': 1, 'circumst': 1, 'use': 1, 'arrang': 1, 'list': 1, 'dead': 1, 'had': 1, 'seseseko': 1, 'high': 1, 'known': 1, 'form': 1, 'get': 1, 'group': 1, 'said': 1, 'nzanga': 1, 'listmast': 1, 'die': 1, 'billion': 1, 'into': 1, 'engag': 1, 'but': 1, 'escap': 1, 'trust': 1, 'howev': 1, 'self': 1, 'becaus': 1, 'discuss': 1, 'sincer': 1, 'famili': 1, 'advisebi': 1, 'are': 1, 'confidenti': 1, 'cancer': 1, 'furnish': 1, 'when': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for idx, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(idx)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1)) # a[rows, col] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15,   0,   0,   0,   0,   0,   0,   0,   6,   0,   0],\n",
       "       [ 75,   3,   2,   1,   1,   1,   0,   1,   0,   0,   0],\n",
       "       [335,  14,  11,  16,  18,  14,  15,   9,   4,  13,   9]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.9208333333333333, total=   0.3s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.8979166666666667, total=   0.1s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................. , score=0.91875, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.9270833333333334, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.9208333333333333, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9170833333333333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=5, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit vs fit-transform = fit computes mean etc on data. transform actually applies changes to data. you can fit the\n",
    "# data and apply transformation to many other datarsets\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
